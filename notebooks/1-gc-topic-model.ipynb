{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Basic Topic Model from the (Generated) ATS Document Database\n",
    "\n",
    "Here I explore the ATS document database --- which should have already been scraped, cleaned, and compiled by scripts held in `/src/`, and managed via `make scrape` and `make data` --- and then train a basic topic model on the corpus. I explore the resulting topics briefly, and show changes by year of meeting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by meeting some basic requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: pandas in /home/g/.local/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/g/.local/lib/python3.8/site-packages (from pandas) (1.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: numpy in /home/g/.local/lib/python3.8/site-packages (1.21.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: wordcloud in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/g/.local/lib/python3.8/site-packages (from wordcloud) (1.21.0)\n",
      "Requirement already satisfied: matplotlib in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from wordcloud) (3.4.2)\n",
      "Requirement already satisfied: pillow in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from wordcloud) (8.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: gensim==3.8.3 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/g/.local/lib/python3.8/site-packages (from gensim==3.8.3) (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/g/.local/lib/python3.8/site-packages (from gensim==3.8.3) (1.21.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from gensim==3.8.3) (1.16.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/g/.local/lib/python3.8/site-packages (from gensim==3.8.3) (3.0.0)\n",
      "Requirement already satisfied: requests in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim==3.8.3) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (4.0.0)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: seaborn in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/g/.local/lib/python3.8/site-packages (from seaborn) (1.21.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/g/.local/lib/python3.8/site-packages (from seaborn) (1.7.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/g/.local/lib/python3.8/site-packages (from seaborn) (1.2.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from seaborn) (3.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (8.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: six in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: matplotlib in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (3.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib) (8.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/g/.local/lib/python3.8/site-packages (from matplotlib) (1.21.0)\n",
      "Requirement already satisfied: six in /home/g/.pyenv/versions/3.8.2/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install wordcloud\n",
    "!pip install gensim==3.8.3\n",
    "!pip install seaborn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also be using `mallet` (via the `gensim` wrapper), so we need to make sure that's installed for use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = './mallet-2.0.8/bin/mallet' # for use when actually passing an LDAMallet wrapper to gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the requirements for loading the data and visualising the initial exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/processed/documents/ats_documents.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe was generated by a feature processing script (which should have been done executed via `Make`, see above). It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm the number of empty text fields seen above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(df[df.raw_text.isnull()])} of {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove these null-valued values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df[df.raw_text.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered by language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['e','f','s','r']:\n",
    "    print(f'{language}: {len(clean_df[clean_df.paper_language_abbreviation == language])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the english documents for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_clean_df = clean_df[clean_df.paper_language_abbreviation == 'e']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a lower case version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # load the regular expression library\n",
    "\n",
    "english_clean_df['processed_text'] = english_clean_df['raw_text'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate an obligatory wordcloud for the whole document corpus as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# list of texts:\n",
    "all_text = ','.join(list(english_clean_df['processed_text'].values))\n",
    "\n",
    "# wordcloud object:\n",
    "wordcloud = WordCloud(background_color='white', max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# generate:\n",
    "wordcloud.generate(all_text)\n",
    "\n",
    "# viz:\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This is, predictably, uninformative.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, similarly, get a sense of the total number of papers per year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(x='meeting_year', data=english_clean_df, orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtered for (only) working papers is slightly more informative here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(x='meeting_year', data=english_clean_df[english_clean_df.paper_type_abbreviation == 'wp'], orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for the LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the `nltk` and its stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need levenshtein distance for later viz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary pre-processing and model-building util wrappers from `gensim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.wrappers import ldamallet\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `spacy` for lemmatization utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a list of all the document texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(english_clean_df.processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build trigram and bigram models from those texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data, min_count=20, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a preprocessing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only need the spacy tagger; there's no need for parser and named entity recognizer, for faster implementation\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# we load the english-language stopword corpus from nltk library\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def process_words(texts, stop_words=stop_words, allowed_tags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Convert a document into a list of lowercase tokens, build bigrams-trigrams, implement lemmatization\"\"\"\n",
    "    # remove stopwords, short tokens and letter accents \n",
    "    texts = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts]\n",
    "    \n",
    "    # bi-gram and tri-gram implementation\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    texts_out = []\n",
    "    \n",
    "    # implement lemmatization and filter out unwanted part of speech tags\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_tags])\n",
    "    \n",
    "    # remove stopwords and short tokens again after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts_out]    \n",
    "    \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this defined, we can actually call the preprocessing function on our list of texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready = process_words(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full dictionary for the vocab of the corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_ready) \n",
    "print('Total Vocabulary Size:', len(id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this contains a large amount of repetition and noise, which we'll deal with below (via thresholds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus, implemented as a bag of words for each text in the above (preprocessed) data list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in data_ready]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the words/frequencies as a dataframe (as a sanity check:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_corpus = {}\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "  for idx, freq in corpus[i]:\n",
    "    if id2word[idx] in dict_corpus:\n",
    "      dict_corpus[id2word[idx]] += freq\n",
    "    else:\n",
    "       dict_corpus[id2word[idx]] = freq\n",
    "       \n",
    "dict_df = pd.DataFrame.from_dict(dict_corpus, orient='index', columns=['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.distplot(dict_df['freq'], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df.sort_values('freq', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the extremely common or extremely uncommon words (above 50% of all documents, or fewer than 10 documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.filter_extremes(no_below=10, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in data_ready]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now got a  much more believeable vocab size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate an Mallet LDA topic model for 100 topics and 500 iterations. **This takes quite a while**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 100\n",
    "n_iterations = 500\n",
    "\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=n_topics, iterations=n_iterations, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate a coherence score for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('Coherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save progress to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../models/ldamallet.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ldamallet, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check topic distributions for the corpus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_results = ldamallet[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most dominant topic of each document in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0] for topics in tm_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 20 significant terms and their probabilities for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [[(term, round(wt, 3)) for term, wt in ldamallet.show_topic(n, topn=20)] for n in range(0, ldamallet.num_topics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A term-topic matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame([[term for term, wt in topic] for topic in topics], columns = ['Term'+str(i) for i in range(1, 21)], index=['Topic '+str(t) for t in range(1, ldamallet.num_topics+1)]).T\n",
    "\n",
    "topics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "def convertldaMalletToldaGen(mallet_model):\n",
    "    model_gensim = LdaModel(\n",
    "        id2word=mallet_model.id2word, num_topics=mallet_model.num_topics,\n",
    "        alpha=mallet_model.alpha) \n",
    "    model_gensim.state.sstats[...] = mallet_model.wordtopics\n",
    "    model_gensim.sync_state()\n",
    "    return model_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldagensim = convertldaMalletToldaGen(ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim as gensimvis\n",
    "\n",
    "vis_data = gensimvis.prepare(ldagensim, corpus, id2word, sort_topics=False)\n",
    "\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominant Topics for Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "corpus_topic_df = pd.DataFrame()\n",
    "\n",
    "# get the Titles from the original dataframe\n",
    "corpus_topic_df['Title'] = english_clean_df['paper_title']\n",
    "\n",
    "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
    "\n",
    "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
    "\n",
    "corpus_topic_df['Topic Terms'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
    "\n",
    "corpus_topic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can show the document counts for each topic and its percentage in the overall corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topic_df = corpus_topic_df.groupby('Dominant Topic').agg(\n",
    "                                  Doc_Count = ('Dominant Topic', np.size),\n",
    "                                  Total_Docs_Perc = ('Dominant Topic', np.size)).reset_index()\n",
    "\n",
    "dominant_topic_df['Total_Docs_Perc'] = dominant_topic_df['Total_Docs_Perc'].apply(lambda row: round((row*100) / len(corpus), 2))\n",
    "\n",
    "dominant_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also which document makes the highest contribution to each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topic_df.groupby(‘Dominant Topic’).apply(lambda topic_set: (topic_set.sort_values(by=[‘Contribution %’], ascending=False).iloc[0])).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic basic weights, using the `tm_results` object defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = pd.DataFrame.from_records([{v: k for v, k in row} for row in tm_results])\n",
    "df_weights.columns = ['Topic ' + str(i) for i in range(1,11)]\n",
    "df_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the year column from the original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights['Year'] = english_clean_df.meeting_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can get an average of yearly topic weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights.groupby('Year').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights['Dominant'] = df_weights.drop('Year', axis=1).idxmax(axis=1)\n",
    "df_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominance = df_weights.groupby('Year')['Dominant'].value_counts(normalize=True).unstack()\n",
    "df_dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meetings = df_weights.groupby(['meeting_type', 'Year'])['Dominant'].value_counts(normalize=True).unstack()\n",
    "\n",
    "df_meetings.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meetings.reset_index(inplace=True)\n",
    "\n",
    "df_melted = df_meetings.melt(id_vars=['meeting_type', 'Year'], value_vars=['Topic ' + str(i) for i in range(1,11)], var_name='Topic', value_name='Prevelance')\n",
    "\n",
    "df_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multiindex dataframe\n",
    "df_meetings.set_index(['Journal', 'Year'], inplace=True)\n",
    "\n",
    "# set the figure size\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "# loop over each meeting type\n",
    "for j in df_meetings.index.levels[0]:\n",
    "  \n",
    "  # get cross-section and plot\n",
    "  df_meetings.xs(j, level=0).plot.area()\n",
    "  \n",
    "  plt.title(j)\n",
    "  plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
